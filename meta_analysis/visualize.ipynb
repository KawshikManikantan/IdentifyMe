{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from utils.get_processed_dataset import get_processed_dataset\n",
    "from utils.utils import read_jsonl, get_major_entities\n",
    "from omegaconf import OmegaConf\n",
    "from configs.config import PRONOUNS_GROUPS, PLURAL_PRONOUNS, dataset_yaml, selected_keys, entity_gender_metadata, pronoun_dialogue_metadata\n",
    "from configs.config_gen import NAME_TO_PREFIX\n",
    "from tqdm.auto import tqdm\n",
    "import hydra\n",
    "from utils.qa_utils import write_qa_to_jsonl, get_mention_info, add_copelands_count\n",
    "import dtale\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path(__file__).resolve().parents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_df_to_heatmap(df, title):\n",
    "    columns = df.columns\n",
    "    unique_labels = sorted(set([re.split(r' - ', col)[0] for col in columns]))\n",
    "    unique_labels.remove(\"model_name\")\n",
    "    for row_ind, row in df.iterrows():\n",
    "        ## Access model_name key of the row\n",
    "        model_name = row[\"model_name\"]\n",
    "        \n",
    "        confusion_matrix = pd.DataFrame(0, index=unique_labels, columns=unique_labels)\n",
    "    \n",
    "        # Populate the confusion matrix from the row data\n",
    "        for col in columns:\n",
    "            if col == \"model_name\":\n",
    "                continue\n",
    "            option_from, option_to = re.split(r' - ', col)\n",
    "            confusion_matrix.at[option_from, option_to] = row[col]\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "        plt.title(f\"{title} - {model_name}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find good examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_path = \"analysis_data/summary/summary_accuracy.xlsx\"\n",
    "count_path = \"analysis_data/summary/summary_count.xlsx\"\n",
    "gender_info_path = \"analysis_data/gender/gender_info.xlsx\"\n",
    "gender_unnested_info_path = \"analysis_data/gender/gender_unnested_info.xlsx\"\n",
    "options_path = \"analysis_data/options/options_info.xlsx\"\n",
    "nested_path = \"analysis_data/nested/nested_info.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(nested_path)\n",
    "# df.rename(columns={\"Unnamed: 0\": \"model_name\"}, inplace=True)\n",
    "# df.to_excel(nested_path, index=False)\n",
    "\n",
    "# print(pd.read_excel(nested_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_unnested_df = pd.read_excel(gender_unnested_info_path)\n",
    "wrap_df_to_heatmap(gender_unnested_df, \"Gender -- Not Nested -- Info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_info_path = \"analysis_data/info/llama3_instruct_mention_info.csv\"\n",
    "mention_info_df = pd.read_csv(mention_info_path)\n",
    "dtale.show(mention_info_df, subprocess=False, host='localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_accuracy_sum = 0.0\n",
    "for row_ind, row in mention_info_df.iterrows():\n",
    "    random_accuracy_sum += 1/row[\"num_options\"]\n",
    "random_accuracy = random_accuracy_sum/len(mention_info_df)\n",
    "print(\"Random Accuracy: \", random_accuracy)\n",
    "\n",
    "random_accuracy_sum = 0.0\n",
    "mention_info_nom_df = mention_info_df[mention_info_df[\"category\"] == \"NOM\"]\n",
    "for row_ind, row in mention_info_nom_df.iterrows():\n",
    "    random_accuracy_sum += 1/row[\"num_options\"]\n",
    "random_accuracy = random_accuracy_sum/len(mention_info_nom_df)\n",
    "print(\"Random Accuracy for Nominals: \", random_accuracy)\n",
    "\n",
    "random_accuracy_sum = 0.0\n",
    "mention_info_pron_df = mention_info_df[mention_info_df[\"category\"] == \"PRON\"]\n",
    "for row_ind, row in mention_info_pron_df.iterrows():\n",
    "    random_accuracy_sum += 1/row[\"num_options\"]\n",
    "random_accuracy = random_accuracy_sum/len(mention_info_pron_df)\n",
    "print(\"Random Accuracy for Pronouns: \", random_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of documents\n",
    "dev_qa_path = \"data/qas/data/qas_dev.jsonl\"\n",
    "test_qa_path = \"data/qas/data/qas_test.jsonl\"\n",
    "dataset_yaml = \"datasets.yaml\"\n",
    "dataset = OmegaConf.load(dataset_yaml)\n",
    "litbank_train_path = dataset.litbank.train_file\n",
    "fantasy_train_path = dataset.fantasy.train_file\n",
    "\n",
    "with jsonlines.open(litbank_train_path) as reader:\n",
    "    litbank_train = [obj[\"doc_key\"] for obj in reader]\n",
    "\n",
    "with jsonlines.open(fantasy_train_path) as reader:\n",
    "    fantasy_train = [obj[\"doc_key\"] for obj in reader]\n",
    "\n",
    "doc_keys = []\n",
    "with jsonlines.open(dev_qa_path) as reader:\n",
    "    dev_qas = list(reader)\n",
    "    for qa in dev_qas:\n",
    "        doc_keys.append(qa[\"doc_key\"])\n",
    "with jsonlines.open(test_qa_path) as reader:\n",
    "    test_qas = list(reader)\n",
    "    for qa in test_qas:\n",
    "        doc_keys.append(qa[\"doc_key\"])\n",
    "\n",
    "print(\"Number of documents: \", len(set(doc_keys)))\n",
    "litbank_count = 0\n",
    "fantasy_count = 0\n",
    "for doc_key in set(doc_keys):\n",
    "    if doc_key in litbank_train:\n",
    "        litbank_count += 1\n",
    "    if doc_key in fantasy_train:\n",
    "        fantasy_count += 1\n",
    "print(\"Litbank documents: \", litbank_count)\n",
    "print(\"Fantasy documents: \", fantasy_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
